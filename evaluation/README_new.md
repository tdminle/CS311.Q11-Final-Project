# RAG Evaluation System

ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t RAG vá»›i 3 metrics chÃ­nh: Basic, Retrieval, vÃ  RAGAS.

## ğŸ“Š Metrics

### 1. Basic Metrics (eval_basic.py)

- **Success Rate**: Tá»· lá»‡ tráº£ lá»i thÃ nh cÃ´ng
- **Response Time**: Thá»i gian pháº£n há»“i trung bÃ¬nh
- **Answer Length**: Äá»™ dÃ i cÃ¢u tráº£ lá»i
- **Sources Used**: Sá»‘ tÃ i liá»‡u sá»­ dá»¥ng

### 2. Retrieval Metrics (eval_retrieval.py)

ÄÃ¡nh giÃ¡ combo Elasticsearch + Qdrant + Reranker:

- **Hit Rate (Recall@5)**: TÃ¬m Ä‘Æ°á»£c tÃ i liá»‡u Ä‘Ãºng trong top 5?
- **MRR**: TÃ i liá»‡u Ä‘Ãºng á»Ÿ vá»‹ trÃ­ nÃ o? (Top 1 = tá»‘t nháº¥t)
- **Context Recall**: TÃ¬m Ä‘á»§ thÃ´ng tin Ä‘á»ƒ tráº£ lá»i?

### 3. RAGAS Metrics (eval_ragas.py)

ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng RAG vá»›i open-source models:

- **Faithfulness**: CÃ¢u tráº£ lá»i cÃ³ trung thá»±c vá»›i context?
- **Answer Relevancy**: CÃ¢u tráº£ lá»i cÃ³ liÃªn quan vá»›i cÃ¢u há»i?
- **Context Precision**: Retrieved contexts cÃ³ chÃ­nh xÃ¡c?
- **Context Recall**: Contexts cÃ³ Ä‘á»§ thÃ´ng tin?

Models: Qwen2.5-3B-Instruct (LLM) + all-MiniLM-L6-v2 (Embeddings)

## ğŸš€ Sá»­ dá»¥ng

```bash
# CÃ i Ä‘áº·t
uv add ragas datasets

# Cháº¡y tá»«ng loáº¡i
uv run python evaluation/eval_basic.py        # Basic metrics
uv run python evaluation/eval_retrieval.py    # Retrieval metrics
uv run python evaluation/eval_ragas.py        # RAGAS metrics
```

## ğŸ“ˆ Káº¿t quáº£

Files lÆ°u trong `evaluation/`:

- `basic_eval_*.json` - Basic metrics
- `retrieval_eval_*.json` - Retrieval metrics
- `ragas_eval_*.json` - RAGAS scores

## ğŸ¯ ÄÃ¡nh giÃ¡ Scores

| Score   | ÄÃ¡nh giÃ¡             |
| ------- | -------------------- |
| â‰¥ 0.8   | âœ… Excellent         |
| 0.6-0.8 | âš ï¸ Good              |
| < 0.6   | âŒ Needs improvement |

## ğŸ’¡ Cáº£i thiá»‡n

- **Low Hit Rate**: Äiá»u chá»‰nh retrieval weights, tÄƒng top_k
- **Low MRR**: Cáº£i thiá»‡n reranking, Ä‘iá»u chá»‰nh ensemble weights
- **Low Faithfulness**: Cáº£i thiá»‡n prompt, giáº£m hallucination
- **Low Relevancy**: Tá»‘i Æ°u system prompt
