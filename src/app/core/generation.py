"""
Generation Module - G·ªçi LLM DeepSeek-R1 ƒë·ªÉ generate c√¢u tr·∫£ l·ªùi
"""
import httpx
from src.app.config import get_settings
from src.app.prompts.templates import build_prompt
from typing import List, Dict, Tuple
import re
from langfuse import observep

settings = get_settings()


@observe(as_type="generation", name="generate_answer")
async def generate_answer(query: str, contexts: List[Dict]) -> Tuple[str, str]:
    """
    Generate c√¢u tr·∫£ l·ªùi d·ª±a tr√™n query v√† contexts s·ª≠ d·ª•ng DeepSeek-R1
    
    Args:
        query: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        contexts: List c√°c context documents ƒë√£ ƒë∆∞·ª£c rerank
    
    Returns:
        Tuple (answer, reasoning) - C√¢u tr·∫£ l·ªùi v√† ph·∫ßn suy lu·∫≠n
    """
    # Build prompt t·ª´ template
    prompt = build_prompt(query, contexts)
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            # G·ªçi API LLM
            response = await client.post(
                settings.LLM_API_URL,
                json={
                    "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.7,
                    "max_tokens": 1024
                },
                headers={"Authorization": f"Bearer {settings.API_KEY}"} if settings.API_KEY else {}
            )
            response.raise_for_status()
            result = response.json()
        
        # Parse response t·ª´ LLM
        # Format th∆∞·ªùng l√†: {"choices": [{"message": {"content": "..."}}]}
        full_response = result["choices"][0]["message"]["content"]
        
        # T√°ch reasoning v√† answer n·∫øu c√≥ tag <thinking>
        reasoning = ""
        answer = full_response
        
        # T√¨m ph·∫ßn reasoning trong <thinking> tags
        thinking_match = re.search(r'<thinking>(.*?)</thinking>', full_response, re.DOTALL)
        if thinking_match:
            reasoning = thinking_match.group(1).strip()
            # Remove thinking part ƒë·ªÉ l·∫•y answer
            answer = re.sub(r'<thinking>.*?</thinking>', '', full_response, flags=re.DOTALL).strip()
        
        print(f"ü§ñ Generated answer (length: {len(answer)} chars)")
        
        return answer, reasoning
    
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi LLM API: {e}")
        return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.", ""
