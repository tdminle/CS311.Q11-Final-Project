# Data Preparation Guide üìö

H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ c√°ch x·ª≠ l√Ω v√† t·∫£i d·ªØ li·ªáu PDF v√†o h·ªá th·ªëng RAG.

## üìÅ C·∫•u tr√∫c Folders

```
my_final_rag/
‚îú‚îÄ‚îÄ data/                      # INPUT: ƒê·∫∑t file PDF ·ªü ƒë√¢y
‚îÇ   ‚îú‚îÄ‚îÄ law_doc_1.pdf
‚îÇ   ‚îú‚îÄ‚îÄ law_doc_2.pdf
‚îÇ   ‚îî‚îÄ‚îÄ law_doc_3.pdf
‚îÇ
‚îú‚îÄ‚îÄ output_data/               # OUTPUT: JSON files (auto-generated)
‚îÇ   ‚îú‚îÄ‚îÄ combined_output.json  # File t·ªïng h·ª£p t·∫•t c·∫£
‚îÇ   ‚îú‚îÄ‚îÄ law_doc_1.json        # (Optional) Output ri√™ng
‚îÇ   ‚îî‚îÄ‚îÄ law_doc_2.json
‚îÇ
‚îî‚îÄ‚îÄ data_preperation/          # Scripts x·ª≠ l√Ω
    ‚îú‚îÄ‚îÄ processing.py          # PDF ‚Üí JSON
    ‚îî‚îÄ‚îÄ load_data.py           # JSON ‚Üí Qdrant/ES
```

## üöÄ Quick Start

### B∆∞·ªõc 1: Chu·∫©n b·ªã PDF files

ƒê·∫∑t t·∫•t c·∫£ file PDF c·∫ßn x·ª≠ l√Ω v√†o folder `data/`:

```bash
# Copy file PDF v√†o folder data
cp /path/to/your/*.pdf data/
```

### B∆∞·ªõc 2: X·ª≠ l√Ω PDF ‚Üí JSON

```bash
# C√†i ƒë·∫∑t dependencies (n·∫øu ch∆∞a c√≥)
pip install PyPDF2 pymupdf langchain-text-splitters

# Ch·∫°y processing service
python data_preperation/processing.py
```

**Output m·∫´u:**

```
============================================================
üöÄ PDF Processing Service
============================================================
Input folder: data
Output folder: output_data
Found 3 PDF file(s)

üìÑ Processing: data/law_doc_1.pdf
  ‚úì Extracted 125000 characters
  ‚úì Created 150 initial chunks
  ‚úì Final chunks: 180

üìÑ Processing: data/law_doc_2.pdf
  ‚úì Extracted 98000 characters
  ‚úì Created 120 initial chunks
  ‚úì Final chunks: 145

‚úÖ Combined output saved to: output_data/combined_output.json

============================================================
üìä Processing Summary:
  Processed: 3/3 files
  Total chunks: 325
============================================================
```

### B∆∞·ªõc 3: T·∫£i v√†o Vector Stores

```bash
# ƒê·∫£m b·∫£o Qdrant v√† Elasticsearch ƒëang ch·∫°y
docker-compose up -d

# Load data
python data_preperation/load_data.py
```

**Output m·∫´u:**

```
============================================================
üöÄ Vietnamese Law Data Loader
============================================================
üìÅ Found combined output: output_data/combined_output.json
üìÇ Loading data from output_data/combined_output.json
‚úÖ Loaded 325 items

üîµ Loading into Qdrant collection: Law
  Initializing embeddings...
  Creating collection...
  Generating embeddings and uploading...
  Processed 325/325 documents
‚úÖ Uploaded 325 points to Qdrant

üü° Loading into Elasticsearch index: law_documents
  Creating index...
  Uploading documents...
  Uploaded 325/325 documents
‚úÖ Uploaded 325 documents to Elasticsearch

============================================================
üîç Verification:
  Qdrant 'Law': 325 points
  Elasticsearch 'law_documents': 325 documents
============================================================
‚úÖ Done!
```

## üîß PDFProcessingService API

### Kh·ªüi t·∫°o Service

```python
from data_preperation.processing import PDFProcessingService

service = PDFProcessingService(
    extraction_method="fitz",  # "pypdf2" ho·∫∑c "fitz" (PyMuPDF)
    max_chunk_length=800       # ƒê·ªô d√†i t·ªëi ƒëa m·ªói chunk
)
```

### X·ª≠ l√Ω m·ªôt file PDF

```python
# X·ª≠ l√Ω m·ªôt file
chunks = service.process_single_pdf("data/my_law.pdf")
print(f"Created {len(chunks)} chunks")

# L∆∞u output
service.save_to_json(chunks, "output_data/my_law.json")
```

### X·ª≠ l√Ω nhi·ªÅu PDFs t·ª´ folder

```python
# X·ª≠ l√Ω t·∫•t c·∫£ v√† t·∫°o file combined
stats = service.process_folder(
    input_folder="data",
    output_folder="output_data",
    combine_output=True  # T·∫°o combined_output.json
)

print(f"Processed: {stats['processed_files']}")
print(f"Total chunks: {stats['total_chunks']}")
```

### X·ª≠ l√Ω v·ªõi output ri√™ng l·∫ª

```python
# M·ªói PDF ‚Üí 1 JSON file ri√™ng
stats = service.process_folder(
    input_folder="data",
    output_folder="output_data",
    combine_output=False  # Kh√¥ng t·∫°o combined file
)
```

## üìä Output Format

### Structure c·ªßa JSON chunks

```json
[
  {
    "title": "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh Ch∆∞∆°ng I NH·ªÆNG QUY ƒê·ªäNH CHUNG",
    "context": "Lu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ b·∫£o ƒë·∫£m tr·∫≠t t·ª±, an to√†n giao th√¥ng ƒë∆∞·ªùng b·ªô..."
  },
  {
    "title": "ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng Ch∆∞∆°ng I NH·ªÆNG QUY ƒê·ªäNH CHUNG",
    "context": "Lu·∫≠t n√†y √°p d·ª•ng ƒë·ªëi v·ªõi t·ªï ch·ª©c, c√° nh√¢n tham gia giao th√¥ng..."
  }
]
```

## ‚öôÔ∏è T√πy ch·ªânh Processing

### Thay ƒë·ªïi extraction method

```python
# S·ª≠ d·ª•ng PyPDF2 (nhanh h∆°n nh∆∞ng k√©m ch√≠nh x√°c)
service = PDFProcessingService(extraction_method="pypdf2")

# S·ª≠ d·ª•ng PyMuPDF/fitz (ch√≠nh x√°c h∆°n, khuy·∫øn ngh·ªã)
service = PDFProcessingService(extraction_method="fitz")
```

### Thay ƒë·ªïi chunk size

```python
# Chunks l·ªõn h∆°n (t·ªëi ƒëa 1000 k√Ω t·ª±)
service = PDFProcessingService(max_chunk_length=1000)

# Chunks nh·ªè h∆°n (t·ªëi ƒëa 500 k√Ω t·ª±)
service = PDFProcessingService(max_chunk_length=500)
```

## üîç Load Data Service

### T√¨m file JSON t·ª± ƒë·ªông

```python
from data_preperation.load_data import find_latest_json

# T·ª± ƒë·ªông t√¨m combined_output.json ho·∫∑c file m·ªõi nh·∫•t
json_file = find_latest_json("output_data")
print(f"Found: {json_file}")
```

### Load JSON data

```python
from data_preperation.load_data import load_json_data

data = load_json_data("output_data/combined_output.json")
print(f"Loaded {len(data)} chunks")
```

### Load v√†o Qdrant

```python
from data_preperation.load_data import load_to_qdrant

data = load_json_data("output_data/combined_output.json")
load_to_qdrant(data, collection_name="Law")
```

### Load v√†o Elasticsearch

```python
from data_preperation.load_data import load_to_elasticsearch

data = load_json_data("output_data/combined_output.json")
load_to_elasticsearch(data, index_name="law_documents")
```

## üõ†Ô∏è Troubleshooting

### L·ªói: No module named 'PyPDF2'

```bash
pip install PyPDF2 pymupdf langchain-text-splitters
```

### L·ªói: No PDF files found

ƒê·∫£m b·∫£o c√≥ file PDF trong folder `data/`:

```bash
ls data/*.pdf
```

### L·ªói: No JSON files found

Ch·∫°y processing tr∆∞·ªõc:

```bash
python data_preperation/processing.py
```

### L·ªói: Connection refused (Qdrant/ES)

```bash
# Ki·ªÉm tra Docker
docker-compose ps

# Kh·ªüi ƒë·ªông services
docker-compose up -d
```

## üìù Best Practices

### 1. T·ªï ch·ª©c file PDF

```
data/
‚îú‚îÄ‚îÄ lu·∫≠t_giao_th√¥ng.pdf
‚îú‚îÄ‚îÄ ngh·ªã_ƒë·ªãnh_100.pdf
‚îî‚îÄ‚îÄ th√¥ng_t∆∞_41.pdf
```

### 2. Naming convention

- S·ª≠ d·ª•ng t√™n file c√≥ √Ω nghƒ©a
- Tr√°nh k√Ω t·ª± ƒë·∫∑c bi·ªát
- D√πng d·∫•u g·∫°ch d∆∞·ªõi thay kho·∫£ng tr·∫Øng

### 3. Workflow chu·∫©n

```bash
# 1. Chu·∫©n b·ªã data
cp *.pdf data/

# 2. Process PDFs
python data_preperation/processing.py

# 3. Verify output
ls output_data/

# 4. Start services (n·∫øu ch∆∞a ch·∫°y)
docker-compose up -d

# 5. Load v√†o database
python data_preperation/load_data.py
```

## üìà Performance Tips

### Large PDF files

V·ªõi PDF l·ªõn (>100 trang):

- S·ª≠ d·ª•ng `extraction_method="fitz"` (ch√≠nh x√°c h∆°n)
- TƒÉng `max_chunk_length` l√™n 1000-1500

### Many PDF files

V·ªõi nhi·ªÅu files (>10 PDFs):

- S·ª≠ d·ª•ng `combine_output=True` ƒë·ªÉ d·ªÖ qu·∫£n l√Ω
- Xem x√©t x·ª≠ l√Ω theo batch n·∫øu qu√° nhi·ªÅu

### Memory optimization

```python
# Process theo batch nh·ªè
import os
from pathlib import Path

pdf_files = list(Path("data").glob("*.pdf"))
batch_size = 5

for i in range(0, len(pdf_files), batch_size):
    batch = pdf_files[i:i+batch_size]
    # Process batch...
```

## üîó Related Documentation

- [Main README](../README.md)
- [Processing Service Code](processing.py)
- [Load Data Service Code](load_data.py)

---

**Need help?** Check the main [README.md](../README.md) or review the source code!
